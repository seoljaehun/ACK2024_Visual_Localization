import torch
import cv2
import numpy as np
import os
import os.path as osp
from mmseg.apis import inference_segmentor, init_segmentor
import mmcv
from sklearn.cluster import DBSCAN
from PIL import Image
from mmseg.datasets.builder import DATASETS
from mmseg.datasets.custom import CustomDataset
from mmcv import Config

def process_image_set(image_set, input_dir, output_dir, model_ckpt_b, model_ckpt_r):
    img_1 = mmcv.imread(os.path.join(input_dir, f"{image_set}_0.png"))
    img_2 = mmcv.imread(os.path.join(input_dir, f"{image_set}_1.png"))
    template_path = os.path.join(input_dir, f"{image_set}_1.png")
    tem_or_path = os.path.join(input_dir, f"{image_set}_2.png")
    original_path = os.path.join(input_dir, f"{image_set}_0.png")

    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
    result_b1 = inference_segmentor(model_ckpt_b, img_1)
    result_r1 = inference_segmentor(model_ckpt_r, img_1)

    # 4ï¸âƒ£ ë§ˆìŠ¤í¬ ë³€í™˜
    mask_b1 = np.array(result_b1[0])  # ê±´ë¬¼ ë§ˆìŠ¤í¬
    mask_r1 = np.array(result_r1[0])  # ë„ë¡œ ë§ˆìŠ¤í¬

    # 5ï¸âƒ£ ë¹ˆ RGB ë§ˆìŠ¤í¬ ìƒì„± (ë°°ê²½: ê²€ì •)
    mask_color_1 = np.zeros((mask_b1.shape[0], mask_b1.shape[1], 3), dtype=np.uint8)

    # 6ï¸âƒ£ ê±´ë¬¼ ë§ˆìŠ¤í¬ ì ìš© (í•˜ëŠ˜ìƒ‰: 135, 206, 235)
    mask_color_1[mask_b1 > 0] = (51, 204, 255)

    # 7ï¸âƒ£ ë„ë¡œ ë§ˆìŠ¤í¬ ì ìš© (ì£¼í™©ìƒ‰: 255, 140, 0), ê±´ë¬¼ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì¡°ê±´ ì¶”ê°€
    mask_color_1[(mask_r1 > 0) & (mask_b1 == 0)] = (255, 165, 0)

        # 3ï¸âƒ£ ì´ë¯¸ì§€ ì…ë ¥ (ì™¸ë¶€ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê¸°ì¡´ img ì‚¬ìš©)
    result_b2 = inference_segmentor(model_ckpt_b, img_2)
    result_r2 = inference_segmentor(model_ckpt_r, img_2)

    # 4ï¸âƒ£ ë§ˆìŠ¤í¬ ë³€í™˜
    mask_b2 = np.array(result_b2[0])  # ê±´ë¬¼ ë§ˆìŠ¤í¬
    mask_r2 = np.array(result_r2[0])  # ë„ë¡œ ë§ˆìŠ¤í¬

    # 5ï¸âƒ£ ë¹ˆ RGB ë§ˆìŠ¤í¬ ìƒì„± (ë°°ê²½: ê²€ì •)
    mask_color_2 = np.zeros((mask_b2.shape[0], mask_b2.shape[1], 3), dtype=np.uint8)

    # 6ï¸âƒ£ ê±´ë¬¼ ë§ˆìŠ¤í¬ ì ìš© (í•˜ëŠ˜ìƒ‰: 135, 206, 235)
    mask_color_2[mask_b2 > 0] = (51, 204, 255)

    # 7ï¸âƒ£ ë„ë¡œ ë§ˆìŠ¤í¬ ì ìš© (ì£¼í™©ìƒ‰: 255, 140, 0), ê±´ë¬¼ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ì¡°ê±´ ì¶”ê°€
    mask_color_2[(mask_r2 > 0) & (mask_b2 == 0)] = (255, 165, 0)

    # ì›ë³¸ ì´ë¯¸ì§€ì™€ í…œí”Œë¦¿ ì´ë¯¸ì§€ ì½ê¸°
    original_image_color = cv2.imread(original_path)  # ì»¬ëŸ¬ ì´ë¯¸ì§€
    tem_or_image_color = cv2.imread(tem_or_path)  # ì»¬ëŸ¬ ì´ë¯¸ì§€ (ì›ë³¸ê³¼ ë™ì¼í•œ í¬ê¸°ì¼ ê²½ìš°)
    original_image = cv2.cvtColor(tem_or_image_color, cv2.COLOR_BGR2GRAY)  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    template_image = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)

    # ì´ë¯¸ì§€ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if original_image_color is None or template_image is None:
        print("Error: ì´ë¯¸ì§€ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit()

    # SIFT íŠ¹ì§•ì  ì¶”ì¶œê¸° ìƒì„±
    sift = cv2.SIFT_create()

    # íŠ¹ì§•ì ê³¼ ë””ìŠ¤í¬ë¦½í„° ê³„ì‚°
    keypoints1, descriptors1 = sift.detectAndCompute(original_image, None)
    keypoints2, descriptors2 = sift.detectAndCompute(template_image, None)

    # íŠ¹ì§•ì  ë””ìŠ¤í¬ë¦½í„° ë§¤ì¹­ (KNN ë§¤ì¹­ ì‚¬ìš©)
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)
    knn_matches = bf.knnMatch(descriptors1, descriptors2, k=2)

    # ì¢‹ì€ ë§¤ì¹­ ì„ íƒ (Lowe's Ratio Test)
    good_matches = []
    for m, n in knn_matches:
        if m.distance < 0.75 * n.distance:
            good_matches.append(m)

    # ë§¤ì¹­ëœ ì ì˜ ìµœì†Œ ê°œìˆ˜ í™•ì¸
    if len(good_matches) > 4:
        src_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)

        # í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚°
        matrix, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

        if matrix is not None:
            h, w = template_image.shape
            pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
            dst = cv2.perspectiveTransform(pts, matrix)

            # ì‚¬ê°í˜•ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì¢Œìƒë‹¨ (x_min, y_min)ê³¼ ìš°í•˜ë‹¨ (x_max, y_max) ì¢Œí‘œ ì°¾ê¸°
            x_min_r, y_min_r = np.int32(dst.min(axis=0).ravel())
            x_max_r, y_max_r = np.int32(dst.max(axis=0).ravel())

            # ì›ë³¸ ì»¬ëŸ¬ ì´ë¯¸ì§€ì— ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
            original_image_with_rect = original_image_color.copy()
            cv2.rectangle(original_image_with_rect, (x_min_r, y_min_r), (x_max_r, y_max_r), (0, 0, 255), 3)

        else:
            print("í˜¸ëª¨ê·¸ë˜í”¼ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§¤ì¹­ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        print("ë§¤ì¹­ëœ íŠ¹ì§•ì ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # 1ï¸âƒ£ ê±´ë¬¼ ë° ë„ë¡œ ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (RGB)
    mask_color_1_gray = cv2.cvtColor(mask_color_1, cv2.COLOR_BGR2GRAY)  # ì›ë³¸ ë§ˆìŠ¤í¬
    mask_color_2_gray = cv2.cvtColor(mask_color_2, cv2.COLOR_BGR2GRAY)  # í…œí”Œë¦¿ ë§ˆìŠ¤í¬

    # SIFT íŠ¹ì§•ì  ì¶”ì¶œê¸° ìƒì„±
    sift = cv2.SIFT_create()

    # íŠ¹ì§•ì ê³¼ ë””ìŠ¤í¬ë¦½í„° ê³„ì‚°
    keypoints1, descriptors1 = sift.detectAndCompute(mask_color_1_gray, None)
    keypoints2, descriptors2 = sift.detectAndCompute(mask_color_2_gray, None)

    # BFMatcherë¥¼ ì´ìš©í•œ KNN ë§¤ì¹­
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)
    knn_matches = bf.knnMatch(descriptors1, descriptors2, k=2)

    # Lowe's Ratio Test ì ìš©
    good_matches = []
    for m, n in knn_matches:
        if m.distance < 0.75 * n.distance:
            good_matches.append(m)

    # ë§¤ì¹­ëœ ì ì´ ì¶©ë¶„í•œì§€ í™•ì¸
    if len(good_matches) > 4:
        src_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        # ğŸ”¹ í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ ê³„ì‚° (ì˜¤ì°¨ë¥¼ ë” í—ˆìš©)
        matrix, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 10.0)  # reprojectionError ì¦ê°€ (ê¸°ì¡´ 5.0 â†’ 10.0)

        if matrix is not None:
            h, w = mask_color_2_gray.shape
            pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
            dst = cv2.perspectiveTransform(pts, matrix)

            # ğŸ”¹ 1ì°¨ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚°
            min_x = max(int(np.min(dst[:, 0, 0])), 0)
            min_y = max(int(np.min(dst[:, 0, 1])), 0)
            max_x = min(int(np.max(dst[:, 0, 0])), mask_color_1_gray.shape[1])
            max_y = min(int(np.max(dst[:, 0, 1])), mask_color_1_gray.shape[0])

            # ğŸ”¹ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ 10% í™•ì¥í•˜ì—¬ ìœ ì—°í•œ íƒìƒ‰ ê°€ëŠ¥í•˜ë„ë¡ ì¡°ì •
            expand_ratio = 4  # 10% í™•ì¥
            dx = int((max_x - min_x) * expand_ratio)
            dy = int((max_y - min_y) * expand_ratio)

            min_x = max(min_x - dx, 0)
            min_y = max(min_y - dy, 0)
            max_x = min(max_x + dx, mask_color_1.shape[1] - 1)
            max_y = min(max_y + dy, mask_color_1.shape[0] - 1)

            # ğŸ”¹ ROIê°€ ìœ íš¨í•œ í¬ê¸°ì¸ì§€ í™•ì¸
            if max_y > min_y and max_x > min_x:
                roi = mask_color_1_gray[min_y:max_y, min_x:max_x]
                roi_color = mask_color_1[min_y:max_y, min_x:max_x]

                if roi.size == 0:
                    print("ROIê°€ ë¹„ì–´ ìˆìŒ: 2ì°¨ ë§¤ì¹­ì„ ì§„í–‰í•˜ì§€ ì•ŠìŒ")
                else:
                    # 2ì°¨ ë§¤ì¹­: ROIì—ì„œ ë‹¤ì‹œ SIFT ì ìš©
                    keypoints_roi, descriptors_roi = sift.detectAndCompute(roi, None)

                    # BFMatcherë¥¼ ì´ìš©í•œ 2ì°¨ KNN ë§¤ì¹­
                    if descriptors_roi is not None and len(descriptors_roi) > 0:
                        knn_matches_2 = bf.knnMatch(descriptors_roi, descriptors2, k=2)

                        # Lowe's Ratio Test ì ìš©
                        good_matches_2 = []
                        for m, n in knn_matches_2:
                            if m.distance < 0.75 * n.distance:
                                good_matches_2.append(m)

                        if len(good_matches_2) > 4:
                            match_pts = np.float32([keypoints_roi[m.queryIdx].pt for m in good_matches_2])

                            # DBSCANì„ ì´ìš©í•˜ì—¬ ë°€ì§‘ëœ ì˜ì—­(í´ëŸ¬ìŠ¤í„°) ì°¾ê¸°
                            dbscan = DBSCAN(eps=20, min_samples=3).fit(match_pts)
                            labels = dbscan.labels_

                            # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°(ìµœë‹¤ ìƒ˜í”Œì´ í¬í•¨ëœ í´ëŸ¬ìŠ¤í„°) ì°¾ê¸°
                            unique_labels, counts = np.unique(labels, return_counts=True)
                            largest_cluster_label = unique_labels[np.argmax(counts[:-1])]  # -1ì€ ë…¸ì´ì¦ˆ ì œì™¸

                            # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ì ë“¤ ì„ íƒ
                            cluster_pts = match_pts[labels == largest_cluster_label]

                            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê³„ì‚°
                            center_x = int(np.mean(cluster_pts[:, 0])) + min_x
                            center_y = int(np.mean(cluster_pts[:, 1])) + min_y

                            # ğŸ”¹ 2ì°¨ ë°”ìš´ë”© ë°•ìŠ¤ ì„¤ì •
                            min_x_final = max(center_x - (x_max_r - x_min_r) // 2, 0)
                            min_y_final = max(center_y - (y_max_r - y_min_r) // 2, 0)
                            max_x_final = min(center_x + (x_max_r - x_min_r) // 2, mask_color_1.shape[1] - 1)
                            max_y_final = min(center_y + (y_max_r - y_min_r) // 2, mask_color_1.shape[0] - 1)

                        # ğŸŸ¢ 3ï¸âƒ£ `road.png` ë¶ˆëŸ¬ì˜¤ê¸°
                            road_img = cv2.imread(original_path)

                    # í¬ê¸° ë§ì¶”ê¸° (mask_color_1ê³¼ ë™ì¼í•œ í¬ê¸°ë¡œ ì¡°ì •)
                            road_img_resized = cv2.resize(road_img, (mask_color_1.shape[1], mask_color_1.shape[0]))

                    # ğŸŸ¢ 4ï¸âƒ£ `road.png`ì— ë°”ìš´ë”© ë°•ìŠ¤ ì˜¤ë²„ë©
                            #cv2.rectangle(road_img_resized, (min_x, min_y), (max_x, max_y), (255, 0, 0), 3)  # 1ì°¨ ë°”ìš´ë”© ë°•ìŠ¤ (íŒŒë€ìƒ‰)
                            cv2.rectangle(road_img_resized, (min_x_final, min_y_final), (max_x_final, max_y_final), (0, 255, 0), 5)  # 2ì°¨ ë°”ìš´ë”© ë°•ìŠ¤ (ì´ˆë¡ìƒ‰)

                        else:
                            print("2ì°¨ ë§¤ì¹­ëœ íŠ¹ì§•ì  ë¶€ì¡±")
                    else:
                        print("ROIì—ì„œ SIFT íŠ¹ì§•ì  ë¶€ì¡±")
            else:
                print("1ì°¨ ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ, 2ì°¨ ë§¤ì¹­ ë¶ˆê°€ëŠ¥")
        else:
            print("í˜¸ëª¨ê·¸ë˜í”¼ ê³„ì‚° ë¶ˆê°€: ë§¤ì¹­ ì  ë¶€ì¡±")
    else:
        print("ë§¤ì¹­ëœ íŠ¹ì§•ì  ë¶€ì¡±")
    # âœ… ìµœì¢… ì´ë¯¸ì§€ì— ë‘ ê°œì˜ ê²°ê³¼ ì˜¤ë²„ë©
    final_image = original_image_color.copy()

    # ì´ˆë¡ìƒ‰ ë°”ìš´ë”© ë°•ìŠ¤ (ì²« ë²ˆì§¸ ê²°ê³¼)
    cv2.rectangle(final_image, 
                (min_x_final, min_y_final), 
                (max_x_final, max_y_final), 
                (0, 255, 0), 3)  # ì´ˆë¡ìƒ‰

    # ë¹¨ê°„ìƒ‰ ë°”ìš´ë”© ë°•ìŠ¤ (ë‘ ë²ˆì§¸ ê²°ê³¼)
    cv2.rectangle(final_image, 
                (x_min_r, y_min_r), 
                (x_max_r, y_max_r), 
                (0, 0, 255), 3)  # ë¹¨ê°„ìƒ‰

    # âœ… êµì§‘í•© ì˜ì—­ ê³„ì‚°
    #inter_x_min = max(min_x_final, x_min_r)
    #inter_y_min = max(min_y_final, y_min_r)
    #inter_x_max = min(max_x_final, x_max_r)
    #inter_y_max = min(max_y_final, y_max_r)

    # âœ… ê²¹ì¹˜ëŠ” ì˜ì—­ì˜ ë„“ì´ ê³„ì‚°
    #inter_width = max(0, inter_x_max - inter_x_min)
    #inter_height = max(0, inter_y_max - inter_y_min)
    #intersection_area = inter_width * inter_height  # êµì§‘í•© ë„“ì´

    intersection_area = compute_intersect_area(x_min_r, x_max_r, min_x_final, max_x_final, y_min_r, y_max_r, min_y_final, max_y_final)

    # âœ… ë¹¨ê°„ìƒ‰ ë°”ìš´ë”© ë°•ìŠ¤ì˜ ë„“ì´
    red_box_area = (x_max_r - x_min_r) * (y_max_r - y_min_r)

    # âœ… ê²¹ì¹˜ëŠ” ì˜ì—­ ë¹„ìœ¨ ê³„ì‚° (ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ ê¸°ì¤€)
    overlap_percentage = (intersection_area / red_box_area) * 100 if red_box_area > 0 else 0

    print(f"ê²¹ì¹˜ëŠ” ì˜ì—­ ë¹„ìœ¨: {overlap_percentage:.2f}%")

    # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ ì„¤ì •
    output_path = os.path.join(output_dir, f"{image_set}_result.png")

    # ì´ë¯¸ì§€ ì €ì¥
    cv2.imwrite(output_path, final_image)

    print(f"Final result saved at: {output_path}")

    return overlap_percentage

def compute_intersect_area(x1, x2, x3, x4, y1, y2, y3, y4):
        ## case1 ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë²—ì–´ë‚˜ ìˆëŠ” ê²½ìš°

    if x2 < x3:
        return 0

        ## case2 ì™¼ìª½ìœ¼ë¡œ ë²—ì–´ë‚˜ ìˆëŠ” ê²½ìš°
    if x1 > x4:
        return 0

        ## case3 ìœ„ìª½ìœ¼ë¡œ ë²—ì–´ë‚˜ ìˆëŠ” ê²½ìš°
    if  y2 < y3:
        return 0

        ## case4 ì•„ë˜ìª½ìœ¼ë¡œ ë²—ì–´ë‚˜ ìˆëŠ” ê²½ìš°
    if  y1 > y4:
        return 0

    left_up_x = max(x1, x3)
    left_up_y = max(y1, y3)
    right_down_x = min(x2, x4)
    right_down_y = min(y2, y4)

    width = right_down_x - left_up_x
    height =  right_down_y - left_up_y
    
    return width * height

classes_b = ('background', 'building')
palette_b = [[0, 0, 0], [48, 200, 248]]

@DATASETS.register_module()
class Dataset(CustomDataset):
  CLASSES = classes_b
  PALETTE = palette_b
  def __init__(self, split, **kwargs):
    super().__init__(img_suffix='.png', seg_map_suffix='.png', 
                     split=split, **kwargs)
    assert osp.exists(self.img_dir) and self.split is not None

# config íŒŒì¼ì„ ì„¤ì •í•˜ê³ , ë‹¤ìš´ë¡œë“œ ë°›ì€ pretrained ëª¨ë¸ì„ checkpointë¡œ ì„¤ì •. 
config_file = 'C:/Users/kkm/Downloads/mmsegmentation/configs/segformer/segformer_mit-b5_8x1_1024x1024_160k_cityscapes.py'
checkpoint_file = 'C:/Users/kkm/Downloads/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'



cfg_b = Config.fromfile(config_file)

cfg_b.norm_cfg = dict(type='BN', requires_grad=True)
#cfg.model.backbone.norm_cfg = cfg.norm_cfg
cfg_b.model.decode_head.norm_cfg = cfg_b.norm_cfg

cfg_b.device='cuda'

cfg_b.model.decode_head.num_classes = 2

#cfg.data.samples_per_gpu=2
#cfg.data.workers_per_gpu=2

cfg_b.model.decode_head.loss_decode = [dict(type='CrossEntropyLoss', loss_weight = 1.0),
                                     dict(type='FocalLoss', loss_weight = 1.0),
                                     dict(type='LovaszLoss', loss_weight = 1.0, reduction='none')]

cfg_b.img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
cfg_b.crop_size = (512, 512)
cfg_b.train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=cfg_b.crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', **cfg_b.img_norm_cfg),
    dict(type='Pad', size=cfg_b.crop_size, pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg']),
]

cfg_b.val_pipeline = [
                    dict(type='LoadImageFromFile'),
                    dict(
                        type='MultiScaleFlipAug',
                        img_scale=(1024, 1024),
                        flip=False,
                        transforms=[
                                    dict(type='Resize', keep_ratio=True),
                                    dict(type='RandomFlip'),
                                    dict(
                                        type='Normalize',
                                        mean=[123.675, 116.28, 103.53],
                                        std=[58.395, 57.12, 57.375],
                                        to_rgb=True),
                                    dict(type='ImageToTensor', keys=['img']),
                                    dict(type='Collect', keys=['img'])
                                    ]),
                    
]


cfg_b.test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 1024),
        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            #dict(type='ResizeToMultiple', size_divisor=32),
            dict(type='RandomFlip'),
            dict(type='Normalize', **cfg_b.img_norm_cfg),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]

cfg_b.data.samples_per_gpu=1
cfg_b.data.workers_per_gpu=0
cfg_b.data.persistent_workers=False
cfg_b.dataset_type = 'Dataset'
cfg_b.data_root = 'D:/'

cfg_b.data.train.type = 'Dataset'
cfg_b.data.train.data_root = 'D:/'
cfg_b.data.train.img_dir = 'Data_set/building_dataset/img_dir/train'
cfg_b.data.train.ann_dir = 'Data_set/building_dataset/contour_ann_dir/train'
cfg_b.data.train.pipeline = cfg_b.train_pipeline
cfg_b.data.train.split = 'D:/Data_set/building_dataset/mask_dir/train/train_building.txt'

cfg_b.data.val.type = 'Dataset'
cfg_b.data.val.data_root = 'D:/'
cfg_b.data.val.img_dir = 'Data_set/building_dataset/img_dir/train'
cfg_b.data.val.ann_dir = 'Data_set/building_dataset/contour_ann_dir/train'
cfg_b.data.val.pipeline = cfg_b.test_pipeline
cfg_b.data.val.split = 'D:/Data_set/building_dataset/mask_dir/train/val_building.txt'

cfg_b.data.test.type = 'Dataset'
cfg_b.data.test.data_root = 'D:/'
cfg_b.data.test.img_dir = 'Data_set/building_dataset/img_dir/val'
cfg_b.data.test.ann_dir = 'Data_set/building_dataset/contour_ann_dir/val'
cfg_b.data.test.pipeline = cfg_b.test_pipeline
cfg_b.data.test.split = 'D:/Data_set/building_dataset/mask_dir/val/test.txt'

cfg_b.load_from = 'C:/Users/kkm/Downloads/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'

# Set up working dir to save files and logs.
cfg_b.work_dir = 'D:/checkpoint/Segformer/building/'

cfg_b.runner.max_iteTrs = 200
cfg_b.log_config.interval = 100
cfg_b.evaluation.interval = 1000  # ëª¨ë¸ í•™ìŠµì‹œ í‰ê°€ë¥¼ ëª‡ ë²ˆì§¸ iterationë§ˆë‹¤ í•  ê²ƒì¸ì§€ ì§€ì •
cfg_b.checkpoint_config.interval = 1000  # ëª¨ë¸ í•™ìŠµì‹œ í•™ìŠµí•œ ëª¨ë¸ì„ ëª‡ ë²ˆì§¸ iterationë§ˆë‹¤ ì €ì¥í•  ê²ƒì¸ì§€ ì§€ì •

cfg_b.runner = dict(type='IterBasedRunner', max_iters=20000)  # Iterationìœ¼ë¡œ ë™ì‘, Epochë¡œ ë™ì‘í•˜ê²Œ ë³€ê²½í•  ìˆ˜ë„ ìˆìŒ
# cfg.runner = dict(type='EpochBasedRunner', max_epochs=4000)  # Epochë¡œ ë³€ê²½
cfg_b.workflow = [('train', 1)]

# Set seed to facitate reproducing the result
cfg_b.seed = 0
#set_random_seed(0, deterministic=False)
cfg_b.gpu_ids = range(1)

classes_r = ('background', 'road')
palette_r = [[0, 0, 0], [255, 127, 0]]

cfg_r = Config.fromfile(config_file)

cfg_r.norm_cfg = dict(type='BN', requires_grad=True)
#cfg.model.backbone.norm_cfg = cfg.norm_cfg
cfg_r.model.decode_head.norm_cfg = cfg_r.norm_cfg

cfg_r.device='cuda'

cfg_r.model.decode_head.num_classes = 2

#cfg.data.samples_per_gpu=2
#cfg.data.workers_per_gpu=2

cfg_r.model.decode_head.loss_decode = [dict(type='CrossEntropyLoss', loss_weight = 1.0),
                                     dict(type='FocalLoss', loss_weight = 1.0),
                                     dict(type='LovaszLoss', loss_weight = 1.0, reduction='none')]

cfg_r.img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
cfg_r.crop_size = (512, 512)
cfg_r.train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=cfg_r.crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', **cfg_r.img_norm_cfg),
    dict(type='Pad', size=cfg_r.crop_size, pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg']),
]

cfg_r.val_pipeline = [
                    dict(type='LoadImageFromFile'),
                    dict(
                        type='MultiScaleFlipAug',
                        img_scale=(1024, 1024),
                        flip=False,
                        transforms=[
                                    dict(type='Resize', keep_ratio=True),
                                    dict(type='RandomFlip'),
                                    dict(
                                        type='Normalize',
                                        mean=[123.675, 116.28, 103.53],
                                        std=[58.395, 57.12, 57.375],
                                        to_rgb=True),
                                    dict(type='ImageToTensor', keys=['img']),
                                    dict(type='Collect', keys=['img'])
                                    ]),
                    
]


cfg_r.test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 1024),
        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            #dict(type='ResizeToMultiple', size_divisor=32),
            dict(type='RandomFlip'),
            dict(type='Normalize', **cfg_r.img_norm_cfg),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]

cfg_r.data.samples_per_gpu=1
cfg_r.data.workers_per_gpu=0
cfg_r.data.persistent_workers=False
cfg_r.dataset_type = 'Dataset'
cfg_r.data_root = 'D:/'

cfg_r.data.train.type = 'Dataset'
cfg_r.data.train.data_root = 'D:/'
cfg_r.data.train.img_dir = 'Data_set/road_dataset/img_dir/train'
cfg_r.data.train.ann_dir = 'Data_set/road_dataset/contour_ann_dir/train'
cfg_r.data.train.pipeline = cfg_r.train_pipeline
cfg_r.data.train.split = 'D:/Data_set/road_dataset/mask_dir/train/train_road.txt'

cfg_r.data.val.type = 'Dataset'
cfg_r.data.val.data_root = 'D:/'
cfg_r.data.val.img_dir = 'Data_set/road_dataset/img_dir/train'
cfg_r.data.val.ann_dir = 'Data_set/road_dataset/contour_ann_dir/train'
cfg_r.data.val.pipeline = cfg_r.test_pipeline
cfg_r.data.val.split = 'D:/Data_set/road_dataset/mask_dir/train/val_road.txt'

cfg_r.data.test.type = 'Dataset'
cfg_r.data.test.data_root = 'D:/'
cfg_r.data.test.img_dir = 'Data_set/road_dataset/img_dir/val'
cfg_r.data.test.ann_dir = 'Data_set/road_dataset/contour_ann_dir/val'
cfg_r.data.test.pipeline = cfg_r.test_pipeline
cfg_r.data.test.split = 'D:/Data_set/road_dataset/mask_dir/val/test.txt'

cfg_r.load_from = 'C:/Users/kkm/Downloads/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'

# Set up working dir to save files and logs.
cfg_r.work_dir = 'D:/checkpoint/Segformer/road/'

cfg_r.runner.max_iteTrs = 200
cfg_r.log_config.interval = 100
cfg_r.evaluation.interval = 1000  # ëª¨ë¸ í•™ìŠµì‹œ í‰ê°€ë¥¼ ëª‡ ë²ˆì§¸ iterationë§ˆë‹¤ í•  ê²ƒì¸ì§€ ì§€ì •
cfg_r.checkpoint_config.interval = 1000  # ëª¨ë¸ í•™ìŠµì‹œ í•™ìŠµí•œ ëª¨ë¸ì„ ëª‡ ë²ˆì§¸ iterationë§ˆë‹¤ ì €ì¥í•  ê²ƒì¸ì§€ ì§€ì •

cfg_r.runner = dict(type='IterBasedRunner', max_iters=20000)  # Iterationìœ¼ë¡œ ë™ì‘, Epochë¡œ ë™ì‘í•˜ê²Œ ë³€ê²½í•  ìˆ˜ë„ ìˆìŒ
# cfg.runner = dict(type='EpochBasedRunner', max_epochs=4000)  # Epochë¡œ ë³€ê²½
cfg_r.workflow = [('train', 1)]

# Set seed to facitate reproducing the result
cfg_r.seed = 0
#set_random_seed(0, deterministic=False)
cfg_r.gpu_ids = range(1)

# ì‹¤í–‰ ì½”ë“œ
input_dir = "D:/exp/"
output_dir = "D:/results/"
os.makedirs(output_dir, exist_ok=True)

checkpoint_file_b = "D:/checkpoint/Segformer/building/Segformer_building.pth"
checkpoint_file_r = "D:/checkpoint/Segformer/road/Segformer_road.pth"
model_ckpt_b = init_segmentor(cfg_b, checkpoint_file_b, device='cuda:0')
model_ckpt_r = init_segmentor(cfg_r, checkpoint_file_r, device='cuda:0')

image_sets = [f.split('_')[0] for f in os.listdir(input_dir) if f.endswith("_0.png")]
overlap_percentages = []

for image_set in set(image_sets):
    overlap_percentage = process_image_set(image_set, input_dir, output_dir, model_ckpt_b, model_ckpt_r)

    overlap_percentages.append(f"{image_set}: {overlap_percentage:.2f}%")

    print(f"Final result saved at: {output_dir}")

overlap_results_path = os.path.join(output_dir, "overlap_results.txt")

with open(overlap_results_path, "w") as f:
    f.write("\n".join(overlap_percentages))

print(f"Overlap percentages saved at: {overlap_results_path}")